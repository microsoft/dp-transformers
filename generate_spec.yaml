# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

$schema: https://componentsdk.azureedge.net/jsonschema/CommandComponent.json
name: microsoft.fhl.transformers.dp.generate
version: 0.0.7
display_name: "dp-transformers-generation"
type: CommandComponent
description: >-
  https://github.com/microsoft/dp-transformers/

is_deterministic: true

tags:
  git: https://github.com/microsoft/dp-transformers

inputs:
  input_training_file:
    type: AnyDirectory
    description: Training data in csv format

  model_type:
    type: Enum
    default: gpt2
    enum:
      - gpt2
      - gpt2-medium
      - gpt2-large
      - gpt2-xl

  model_name_or_path:
    type: AnyDirectory
    description: Model

  length:
    type: Integer
    default: 256

  total_sequences:
    type: Integer
    default: 120

  batch_size:
    type: Integer
    default: 8

  lora_dim:
    type: Integer
    default: 4
  
  lora_alpha:
    type: Float
    default: 32

  lora_dropout:
    type: Float
    default: 0.0

# Prompt arguments
  prompt_columns:
    type: String
    default: Subject,Importance,HasAttachments
    description: Column names to be used on prompt generation. Will be transformed in a list of strings list[str].
    optional: false

  prompt_string:
    type: String
    default: "Create email UniqueBody with Subject {}, with Importance {} and HasAttachments is {}"
    description: Prompt base string for prompt generation. Prompt will be formed from 'prompt_base_string.format(*prompt_columns_list)'
    optional: false

# Spark Column Transformer Arguments
# Input dataset arguments
  dataset_input_path:
    type: AnyDirectory
    description: Path to dataset files. Accepts datasets with Parquet format. Base path can be provided as reading will recurse through folders.
    optional: false

  dataset_input_format:
    type: String
    default: Parquet
    description: Dataset format. Parquet or Json. Default Parquet.
    optional: true

# Synthetic dataset arguments
  output_format:
    type: String
    default: Parquet
    description: Syntehtic dataset format. Parquet or Json. Default Parquet.
    optional: true
  output_partitions:
    type: Integer
    default: 0
    description: Syntehtic dataset number of partitions. For no repartitioning set to 0. Default 0.
    optional: true

outputs:
  output_dir:
    type: AnyDirectory
    description: "output_dir"

command: >-
  python examples/nlg-reddit/sample-level-dp/generate-text.py
  --model_type {inputs.model_type}
  --model_name_or_path {inputs.model_name_or_path}
  --input_training_file {inputs.input_training_file}
  --output_dir {outputs.output_dir}
  --length {inputs.length}
  --total_sequences {inputs.total_sequences}
  --batch_size {inputs.batch_size}
  --lora_dim {inputs.lora_dim}
  --lora_alpha {inputs.lora_alpha}
  --lora_dropout {inputs.lora_dropout}
  --do_sample
# Prompt arguments
  --prompt_columns {inputs.prompt_columns}
  --prompt_string {inputs.prompt_string}
# Spark Column Transformer Arguments
  --dataset_input_path {inputs.dataset_input_path}
  [--dataset_input_format {inputs.dataset_input_format}]
  [--output_format {inputs.output_format}]
  [--output_partitions {inputs.output_partitions}]

environment:
  docker:
    image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - conda-forge
      dependencies:
      - pip=20.2
      - python=3.7.9
      - pip:
        - shrike
        - pyarrow==11.0.0
        - pandas
        - opacus==1.1.3
        - torch==1.12.1
        - transformers==4.20.1
        - datasets==2.0.0
        - opacus==1.1.3
        - prv-accountant<0.2.0
        - scikit-learn
        - dp-transformers
        - pyspark==3.1.1
  os: Linux