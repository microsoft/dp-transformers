description: opacus-utils-examples-language-modelling

target:
  service: amlk8s
  name: itphyperdgx2cl1
  vc: hai7a

# environment:
#   registry: mcr.microsoft.com
#   image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:latest
#   conda_yaml_file: $CONFIG_DIR/environment.yml # requires amlt>=8.2.1.post1
#   setup:
#     - pip install .

environment:
  registry: mcr.microsoft.com
  image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:latest
  image_setup:
    - pip install torch==1.12.1
    - pip install transformers==4.26.0
    - pip install datasets==2.6.1
  setup:
    - pip install sentencepiece
    - pip install .

code:
  local_dir: $CONFIG_DIR/../../

storage:
  input_path:
    # You should use your own blob here
    storage_account_name: huinan
    container_name: amulet
  output_path:
    # You should use your own blob here
    storage_account_name: huinan
    container_name: amulet

jobs:
  - name: moe-full-nodp-16v100-epochs_100
    sku: 32G16-V100
    command:
      - python -m torch.distributed.run --nproc_per_node 16 examples/moe/fine-tune-nodp-lm.py
        --data_dir /mnt/input_path/huggingface
        --output_dir $$AMLT_OUTPUT_DIR
        --save_strategy no
        --model_name google/switch-base-8
        --sequence_len 128
        --per_device_train_batch_size 1
        --gradient_accumulation_steps 1
        --evaluation_strategy steps
        --eval_steps 2
        --log_level info
        --per_device_eval_batch_size 1
        --eval_accumulation_steps 1
        --seed 42
        --prediction_loss_only
        --weight_decay 0.01
        --remove_unused_columns False
        --num_train_epochs 100
        --logging_steps 1
        --max_grad_norm 0
        --lr_scheduler_type constant
        --learning_rate 1e-4
        --disable_tqdm True
        --dataloader_num_workers 2
        --report_to azure_ml
        $EXTRA_ARGS
    submit_args:
      max_attempts: 1
