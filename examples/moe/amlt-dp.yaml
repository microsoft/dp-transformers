description: moe

target:
  service: amlk8s
  name: itphyperdgx2cl1
  vc: hai7a

# environment:
#   registry: mcr.microsoft.com
#   image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:latest
#   conda_yaml_file: $CONFIG_DIR/environment.yml # requires amlt>=8.2.1.post1
#   setup:
#     - pip install .

environment:
  registry: mcr.microsoft.com
  image: azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:latest
  image_setup:
    - pip install torch==1.12.1
    - pip install transformers==4.26.0
    - pip install datasets==2.6.2
    - pip install evaluate
    - pip install sentencepiece
    - pip install scikit-learn
  setup:
    - pip install .

code:
  local_dir: $CONFIG_DIR/../../

storage:
  input_path:
    # You should use your own blob here
    storage_account_name: huinan
    container_name: amulet
  output_path:
    # You should use your own blob here
    storage_account_name: huinan
    container_name: amulet


# search:
#   job_template:
#     name: moe-full-dp-16v100-mnli_{experiment_name:s}_{auto:4s}
#     sku: 32G16-V100
#     command:
#       - python -m torch.distributed.run --nproc_per_node 16 examples/moe/fine-tune-dp-cls.py
#         --data_dir /mnt/input_path/huggingface
#         --task {task}
#         --output_dir /mnt/input_path/huggingface/moe/mnli/nodp/{experiment_name:s}_{auto:4s}
#         --save_strategy no
#         --model_name google/switch-base-8
#         --sequence_len {sequence_len}
#         --per_device_train_batch_size 2
#         --gradient_accumulation_steps 1
#         --evaluation_strategy steps
#         --eval_steps 250
#         --log_level info
#         --per_device_eval_batch_size 4
#         --eval_accumulation_steps 1
#         --seed 42
#         --weight_decay 0.01
#         --remove_unused_columns False
#         --num_train_epochs {num_train_epochs}
#         --logging_steps 125
#         --max_grad_norm 0
#         --lr_scheduler_type constant
#         --learning_rate {learning_rate}
#         --per_sample_max_grad_norm 1.0
#         --target_epsilon 8.0
#         --disable_tqdm True
#         --dataloader_num_workers 2
#         --report_to azure_ml
#         $EXTRA_ARGS
#   type: grid
#   max_trials: 1
#   params:
#     - name: num_train_epochs
#       values: choice(3)
#     - name: learning_rate
#       values: choice(0.0001)
#     - name: sequence_len
#       values: choice(128)
#     - name: task
#       values: choice("SetFit/mnli")


search:
  job_template:
    name: moe-full-dp-16v100-sst2_{experiment_name:s}_{auto:4s}
    sku: 32G16-V100
    command:
      - python -m torch.distributed.run --nproc_per_node 16 examples/moe/fine-tune-dp-cls.py
        --data_dir /mnt/input_path/huggingface
        --task {task}
        --output_dir /mnt/input_path/huggingface/moe/sst2/nodp/{experiment_name:s}_{auto:4s}
        --save_strategy no
        --model_name google/switch-base-8
        --sequence_len {sequence_len}
        --per_device_train_batch_size 4
        --gradient_accumulation_steps 16
        --evaluation_strategy epoch
        --log_level info
        --per_device_eval_batch_size 8
        --eval_accumulation_steps 1
        --seed 42
        --weight_decay 0.01
        --remove_unused_columns False
        --num_train_epochs {num_train_epochs}
        --logging_steps 5
        --max_grad_norm 0
        --lr_scheduler_type constant
        --learning_rate {learning_rate}
        --per_sample_max_grad_norm 1.0
        --target_epsilon {target_epsilon}
        --disable_tqdm True
        --dataloader_num_workers 2
        --report_to azure_ml
        $EXTRA_ARGS
  type: grid
  max_trials: 1
  params:
    - name: num_train_epochs
      values: choice(20)
    - name: learning_rate
      values: choice(0.0001)
    - name: sequence_len
      values: choice(128)
    - name: task
      values: choice("sst2")
    - name: target_epsilon
      values: choice(8.0)

# jobs:
#   - name: moe-full-nodp-16v100-epochs_100
#     sku: 32G16-V100
#     command:
#       - python -m torch.distributed.run --nproc_per_node 16 examples/moe/fine-tune-nodp-lm.py
#         --data_dir /mnt/input_path/huggingface
#         --output_dir $$AMLT_OUTPUT_DIR
#         --save_strategy no
#         --model_name google/switch-base-8
#         --sequence_len 128
#         --per_device_train_batch_size 1
#         --gradient_accumulation_steps 1
#         --evaluation_strategy steps
#         --eval_steps 2
#         --log_level info
#         --per_device_eval_batch_size 1
#         --eval_accumulation_steps 1
#         --seed 42
#         --prediction_loss_only
#         --weight_decay 0.01
#         --remove_unused_columns False
#         --num_train_epochs 100
#         --logging_steps 1
#         --max_grad_norm 0
#         --lr_scheduler_type constant
#         --learning_rate 1e-4
#         --disable_tqdm True
#         --dataloader_num_workers 2
#         --report_to azure_ml
#         $EXTRA_ARGS
#     submit_args:
#       max_attempts: 1
